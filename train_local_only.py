"""
ğŸš€ KAVA å®Œå…¨æœ¬åœ°åŒ–è®­ç»ƒè„šæœ¬
æ‰€æœ‰æ¨¡å‹å’Œæ•°æ®é›†ä»æœ¬åœ°åŠ è½½ï¼Œå®ç°ç¦»çº¿è®­ç»ƒ
é€‚é… RTX 4070 8GB VRAM
"""

import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from datasets import load_dataset, load_from_disk
from tqdm import tqdm
import os
import sys

# å¼•å…¥æ ¸å¿ƒç»„ä»¶
from experiments.kv_dimension_projector import KVDimensionProjector
from src.losses import MercatorKVLoss

# --- ğŸ”¥ å…¨æœ¬åœ°åŒ–é…ç½® (4070/8GB ä¼˜åŒ–) ---
CONFIG = {
    # âœ… æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆç”± download_local_resources.py ä¸‹è½½ï¼‰
    "teacher_path": "local_models/qwen-1.5b-teacher",          
    "student_path": "local_models/qwen-0.5b-student",
    
    # âœ… æœ¬åœ°æ•°æ®é›†è·¯å¾„ï¼ˆä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²é¿å…è½¬ä¹‰é—®é¢˜ï¼‰
    "dataset_path": r"H:\kava\quickly check\local_data\gsm8k",
    "dataset_split": "train",
    
    # æ˜¾å­˜ä¼˜åŒ–å‚æ•° (8GB VRAM é»„é‡‘é…ç½®)
    "batch_size": 2,          
    "gradient_accumulation_steps": 16,  # ç­‰æ•ˆ Batch 32
    "max_length": 512,
    "lr_projector": 1e-3,
    "lr_student": 5e-5,
    "epochs": 1,
    "save_steps": 200,
    "device": "cuda",
    
    # éªŒè¯é…ç½®
    "verify_local_files": True  # å¯åŠ¨å‰æ£€æŸ¥æœ¬åœ°æ–‡ä»¶
}

def verify_local_resources():
    """éªŒè¯æœ¬åœ°èµ„æºæ˜¯å¦å®Œæ•´"""
    print("ğŸ” Verifying local resources...")
    
    errors = []
    
    # æ£€æŸ¥æ¨¡å‹
    for model_name, path in [("Teacher", CONFIG["teacher_path"]), 
                              ("Student", CONFIG["student_path"])]:
        if not os.path.exists(path):
            errors.append(f"âŒ {model_name} not found: {path}")
        elif not os.path.exists(os.path.join(path, "config.json")):
            errors.append(f"âŒ {model_name} incomplete: missing config.json")
        else:
            print(f"   âœ… {model_name}: {path}")
    
    # æ£€æŸ¥æ•°æ®é›†
    if not os.path.exists(CONFIG["dataset_path"]):
        errors.append(f"âŒ Dataset not found: {CONFIG['dataset_path']}")
    elif not os.path.exists(os.path.join(CONFIG["dataset_path"], "dataset_info.json")):
        errors.append(f"âŒ Dataset incomplete: missing dataset_info.json")
    else:
        print(f"   âœ… Dataset: {CONFIG['dataset_path']}")
    
    if errors:
        print("\nâš ï¸ Local resources verification failed:")
        for error in errors:
            print(f"   {error}")
        print("\nğŸ’¡ Solution: Run download script first:")
        print("   python download_local_resources.py")
        return False
    
    print("   âœ… All local resources verified!\n")
    return True

def extract_flat_kv(past_key_values):
    """æå–æœ€åä¸€å±‚çš„ Key å¹¶å±•å¹³"""
    k, v = past_key_values[-1] 
    B, H, T, D_h = k.shape
    return k.permute(0, 2, 1, 3).contiguous().view(B, T, H * D_h)

def main():
    print("\n" + "ğŸ¯" * 35)
    print("  KAVA Fully Localized Training")
    print("  å®Œå…¨æœ¬åœ°åŒ–è®­ç»ƒï¼ˆç¦»çº¿æ¨¡å¼ï¼‰")
    print("ğŸ¯" * 35 + "\n")
    
    print(f"ğŸš€ Configuration:")
    print(f"   Teacher: {CONFIG['teacher_path']}")
    print(f"   Student: {CONFIG['student_path']}")
    print(f"   Dataset: {CONFIG['dataset_path']}")
    print(f"   Batch: {CONFIG['batch_size']} x {CONFIG['gradient_accumulation_steps']} = {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}")
    
    # éªŒè¯æœ¬åœ°èµ„æº
    if CONFIG["verify_local_files"]:
        if not verify_local_resources():
            sys.exit(1)
    
    # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
    os.makedirs("checkpoints", exist_ok=True)
    
    # --- 1. æ•°æ®åŠ è½½ (ä½¿ç”¨ Arrow æ ¼å¼ç›´æ¥åŠ è½½æœ¬åœ°æ–‡ä»¶) ---
    print(f"ğŸ“š Loading local dataset from: {CONFIG['dataset_path']}")
    
    try:
        # æ–¹æ³• 1: å°è¯•ä½¿ç”¨ load_from_disk (å®Œæ•´å…ƒæ•°æ®)
        try:
            print("   Attempting load_from_disk (method 1)...")
            dataset = load_from_disk(CONFIG["dataset_path"])
            
            # æ™ºèƒ½å¤„ç†æ•°æ®é›†ç»“æ„
            if isinstance(dataset, dict) and CONFIG["dataset_split"] in dataset:
                dataset_to_use = dataset[CONFIG["dataset_split"]]
                print(f"   âœ… Loaded {len(dataset_to_use)} samples from split '{CONFIG['dataset_split']}'")
            elif hasattr(dataset, '__len__'):
                dataset_to_use = dataset
                print(f"   âœ… Loaded {len(dataset_to_use)} samples (single dataset)")
            else:
                raise ValueError(f"Unrecognized dataset structure: {type(dataset)}")
            
            train_data = dataset_to_use
            
        except Exception as e1:
            # æ–¹æ³• 2: å›é€€åˆ° Arrow æ ¼å¼ç›´æ¥åŠ è½½
            print(f"   Method 1 failed ({e1}), trying Arrow format (method 2)...")
            
            local_data_path = CONFIG['dataset_path']
            train_dir = os.path.join(local_data_path, 'train')
            
            # æ£€æŸ¥ train ç›®å½•æ˜¯å¦å­˜åœ¨
            if not os.path.exists(train_dir):
                raise FileNotFoundError(f"Train directory not found: {train_dir}")
            
            # ç›´æ¥è¯»å– Arrow æ–‡ä»¶
            print(f"   Loading Arrow files from: {train_dir}")
            dataset = load_dataset(
                "arrow",
                data_files={'train': f"{train_dir}/*.arrow"},
                split='train'
            )
            
            train_data = dataset
            print(f"   âœ… Loaded {len(train_data)} samples using Arrow format")
        
    except Exception as e:
        print(f"\n   âŒ All dataset loading methods failed!")
        print(f"   Error: {e}")
        print("\n   ğŸ’¡ Troubleshooting:")
        print(f"   1. Check if {CONFIG['dataset_path']} exists")
        print(f"   2. Check if {CONFIG['dataset_path']}/train/*.arrow files exist")
        print("   3. Run: python repair_dataset.py")
        print("   4. Verify dataset structure")
        
        # åˆ—å‡ºç›®å½•å†…å®¹å¸®åŠ©è°ƒè¯•
        if os.path.exists(CONFIG['dataset_path']):
            print(f"\n   ğŸ“‚ Contents of {CONFIG['dataset_path']}:")
            for item in os.listdir(CONFIG['dataset_path']):
                item_path = os.path.join(CONFIG['dataset_path'], item)
                if os.path.isdir(item_path):
                    print(f"      [DIR]  {item}/")
                else:
                    print(f"      [FILE] {item}")
        
        sys.exit(1)
    
    # åŠ è½½åˆ†è¯å™¨ï¼ˆä»æœ¬åœ°ï¼‰
    print("\nğŸ”¤ Loading tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(CONFIG['student_path'])
        if tokenizer.pad_token is None: 
            tokenizer.pad_token = tokenizer.eos_token
        print(f"   âœ… Tokenizer loaded from {CONFIG['student_path']}")
    except Exception as e:
        print(f"   âŒ Tokenizer loading failed: {e}")
        sys.exit(1)
    
    # æ•°æ®é¢„å¤„ç†
    print("\nğŸ”§ Processing dataset...")
    def process(examples):
        texts = [q + "\n" + a for q, a in zip(examples['question'], examples['answer'])]
        return tokenizer(texts, truncation=True, padding="max_length", max_length=CONFIG['max_length'])
    
    tokenized_data = train_data.map(process, batched=True, remove_columns=train_data.column_names)
    tokenized_data.set_format("torch", columns=["input_ids", "attention_mask"])
    dataloader = DataLoader(tokenized_data, batch_size=CONFIG['batch_size'], shuffle=True)
    print(f"   âœ… {len(dataloader)} batches ready")

    # --- 2. æ¨¡å‹åŠ è½½ (å…¨éƒ¨ä»æœ¬åœ°æ–‡ä»¶å¤¹åŠ è½½) ---
    print("\nğŸ¤– Loading models from local disk...")
    
    try:
        # Teacher (4-bité‡åŒ–ï¼ŒèŠ‚çœ VRAM)
        print("   Loading Teacher (4-bit quantized)...")
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_quant_type="nf4"
        )
        teacher = AutoModelForCausalLM.from_pretrained(
            CONFIG['teacher_path'], 
            quantization_config=bnb_config, 
            device_map="auto",
            local_files_only=True  # å¼ºåˆ¶ä»…ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        )
        teacher.eval()
        print(f"      âœ… Teacher loaded: d_model={teacher.config.hidden_size}")
        
        # Student (ä»æœ¬åœ°è·¯å¾„åŠ è½½)
        print("   Loading Student (bfloat16)...")
        student = AutoModelForCausalLM.from_pretrained(
            CONFIG['student_path'], 
            torch_dtype=torch.bfloat16, 
            device_map="auto",
            local_files_only=True  # å¼ºåˆ¶ä»…ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        )
        student.train()
        print(f"      âœ… Student loaded: d_model={student.config.hidden_size}")
        
    except Exception as e:
        print(f"   âŒ Model loading failed: {e}")
        print("\n   ğŸ’¡ Troubleshooting:")
        print("   1. Check if model directories exist")
        print("   2. Run: python download_local_resources.py")
        print("   3. Verify disk space (need ~3-4 GB)")
        sys.exit(1)

    # --- 3. åˆå§‹åŒ– KAVA ç»„ä»¶ ---
    print("\nğŸ—ºï¸ Initializing Map Projection...")
    t_dim = teacher.config.hidden_size
    s_dim = student.config.hidden_size
    
    projector = KVDimensionProjector(
        teacher_configs={"local_teacher": {"d_model": t_dim}},
        student_d_model=s_dim,
        mlp_ratio=1.0,
        dropout=0.1
    ).to(CONFIG['device']).to(torch.bfloat16)
    
    loss_fn = MercatorKVLoss(alpha=1.0, beta=0.01).to(CONFIG['device'])
    
    print(f"   Projector: {t_dim} -> {s_dim}")
    print(f"   Loss: Mercator (alpha=1.0, beta=0.01)")
    
    optimizer = optim.AdamW([
        {'params': student.parameters(), 'lr': CONFIG['lr_student']},
        {'params': projector.parameters(), 'lr': CONFIG['lr_projector']}
    ])
    
    print(f"   Optimizer: Student LR={CONFIG['lr_student']}, Projector LR={CONFIG['lr_projector']}")

    # --- 4. è®­ç»ƒå¾ªç¯ ---
    print("\n" + "=" * 70)
    print("ğŸ¯ Training Start - Monitor 'CosSim' (Target: >0.90)")
    print("=" * 70)
    
    global_step = 0
    progress = tqdm(dataloader, desc="Training")
    
    try:
        for i, batch in enumerate(progress):
            input_ids = batch['input_ids'].to(CONFIG['device'])
            mask = batch['attention_mask'].to(CONFIG['device'])
            
            # Teacher Forward (No Grad)
            with torch.no_grad():
                t_out = teacher(input_ids, attention_mask=mask, use_cache=True)
                t_kv = extract_flat_kv(t_out.past_key_values)
                
            # Student Forward
            s_out = student(input_ids, attention_mask=mask, use_cache=True)
            s_kv = extract_flat_kv(s_out.past_key_values)
            
            # Projection & Loss
            t_proj, _ = projector.project_teacher_kv("local_teacher", t_kv, t_kv)
            loss, metrics = loss_fn(s_kv, t_proj)
            
            # æ¢¯åº¦ç´¯ç§¯
            loss = loss / CONFIG['gradient_accumulation_steps']
            loss.backward()
            
            if (i + 1) % CONFIG['gradient_accumulation_steps'] == 0:
                torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)
                optimizer.step()
                optimizer.zero_grad()
                global_step += 1
                
                # æ ¸å¿ƒç›‘æ§
                actual_loss = loss.item() * CONFIG['gradient_accumulation_steps']
                cos_sim = metrics['cos_sim']
                
                # çŠ¶æ€åˆ¤æ–­
                if cos_sim >= 0.95:
                    status = "âœ… Excellent"
                elif cos_sim >= 0.90:
                    status = "ğŸ¯ Great"
                elif cos_sim >= 0.70:
                    status = "ğŸ“ˆ Good"
                elif cos_sim >= 0.50:
                    status = "âš ï¸ Learning"
                else:
                    status = "ğŸ”„ Adapting"
                
                progress.set_postfix({
                    "Loss": f"{actual_loss:.4f}", 
                    "CosSim": f"{cos_sim:.4f}",
                    "Status": status
                })
                
                # æ¯ 50 æ­¥è¯¦ç»†æŠ¥å‘Š
                if global_step % 50 == 0:
                    print(f"\n[Step {global_step:04d}] Loss: {actual_loss:.4f} | CosSim: {cos_sim:.4f} {status}")
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if global_step % CONFIG['save_steps'] == 0:
                    checkpoint_path = f"checkpoints/proj_step_{global_step}.pth"
                    torch.save(projector.state_dict(), checkpoint_path)
                    print(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")

        print("\n" + "=" * 70)
        print("âœ… Training Complete!")
        print("=" * 70)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        torch.save(projector.state_dict(), "final_projector.pth")
        student.save_pretrained("final_student")
        
        print("ğŸ’¾ Final models saved:")
        print("   - final_projector.pth")
        print("   - final_student/")
        print("\nğŸ‰ All Done!")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Training interrupted by user")
        print("ğŸ’¾ Saving emergency checkpoint...")
        torch.save(projector.state_dict(), "checkpoints/emergency_projector.pth")
        print("âœ… Emergency checkpoint saved")
    
    except Exception as e:
        print(f"\n\nâŒ Error occurred: {e}")
        print("ğŸ’¾ Saving emergency checkpoint...")
        torch.save(projector.state_dict(), "checkpoints/emergency_projector.pth")
        raise

if __name__ == "__main__":
    main()
