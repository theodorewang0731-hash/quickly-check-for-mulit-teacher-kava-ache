"""
ç¯å¢ƒè‡ªé€‚åº”æ¨¡å—
è‡ªåŠ¨æ£€æµ‹å¹¶é€‚é…ä¸åŒçš„è¿è¡Œç¯å¢ƒï¼ˆæœ¬åœ°ã€HPCã€äº‘å¹³å°ï¼‰
ç¡®ä¿ä»£ç åœ¨ä»»ä½•ç¯å¢ƒä¸‹éƒ½èƒ½æ­£ç¡®è¿è¡Œ
"""

import os
import sys
import platform
import subprocess
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
import yaml
import warnings

import torch
import transformers


class EnvironmentAdapter:
    """ç¯å¢ƒè‡ªé€‚åº”å™¨ - è‡ªåŠ¨æ£€æµ‹å¹¶é…ç½®è¿è¡Œç¯å¢ƒ"""
    
    def __init__(self, config_path: Optional[str] = None):
        """
        åˆå§‹åŒ–ç¯å¢ƒé€‚é…å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º configs/environment_config.yaml
        """
        self.project_root = Path(__file__).parent.parent
        
        # åŠ è½½é…ç½®
        if config_path is None:
            config_path = self.project_root / "configs" / "environment_config.yaml"
        self.config = self._load_config(config_path)
        
        # æ£€æµ‹ç¯å¢ƒ
        self.env_info = self._detect_environment()
        
        # é…ç½®ç¡¬ä»¶
        self.hardware_config = self._configure_hardware()
        
        # é…ç½®è·¯å¾„
        self.paths = self._configure_paths()
        
        # æ£€æµ‹ä¾èµ–
        self.dependencies = self._detect_dependencies()
        
    def _load_config(self, config_path: Path) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if not config_path.exists():
            warnings.warn(f"Config file not found: {config_path}, using defaults")
            return self._get_default_config()
        
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'environment_type': 'auto',
            'hardware': {
                'gpu_detection': {'auto_detect': True},
                'precision': {'auto_detect': True}
            },
            'model_dimensions': {'auto_detect': True},
            'paths': {'auto_detect': True},
            'dependencies': {'auto_detect': True}
        }
    
    def _detect_environment(self) -> Dict[str, Any]:
        """
        æ£€æµ‹è¿è¡Œç¯å¢ƒç±»å‹
        
        Returns:
            åŒ…å«ç¯å¢ƒä¿¡æ¯çš„å­—å…¸
        """
        env_info = {
            'type': 'unknown',
            'platform': platform.system(),
            'python_version': sys.version,
            'hostname': platform.node(),
            'cpu_count': os.cpu_count(),
        }
        
        # æ£€æµ‹ HPC ç¯å¢ƒç‰¹å¾
        hpc_indicators = [
            'SLURM_JOB_ID',      # SLURM
            'PBS_JOBID',         # PBS
            'SGE_TASK_ID',       # SGE
            'LSB_JOBID',         # LSF
            'COBALT_JOBID',      # Cobalt
        ]
        
        if any(indicator in os.environ for indicator in hpc_indicators):
            env_info['type'] = 'hpc'
            env_info['scheduler'] = self._detect_scheduler()
        
        # æ£€æµ‹äº‘å¹³å°
        elif 'KUBERNETES_SERVICE_HOST' in os.environ:
            env_info['type'] = 'cloud'
            env_info['platform_type'] = 'kubernetes'
        
        # é»˜è®¤ä¸ºæœ¬åœ°ç¯å¢ƒ
        else:
            env_info['type'] = 'local'
        
        return env_info
    
    def _detect_scheduler(self) -> Optional[str]:
        """æ£€æµ‹ HPC ä½œä¸šè°ƒåº¦å™¨"""
        if 'SLURM_JOB_ID' in os.environ:
            return 'slurm'
        elif 'PBS_JOBID' in os.environ:
            return 'pbs'
        elif 'SGE_TASK_ID' in os.environ:
            return 'sge'
        elif 'LSB_JOBID' in os.environ:
            return 'lsf'
        return None
    
    def _configure_hardware(self) -> Dict[str, Any]:
        """
        é…ç½®ç¡¬ä»¶ï¼ˆGPUã€ç²¾åº¦ã€å†…å­˜ç­‰ï¼‰
        
        Returns:
            ç¡¬ä»¶é…ç½®å­—å…¸
        """
        hw_config = {
            'device': 'cpu',
            'device_name': 'CPU',
            'precision': 'fp32',
            'num_gpus': 0,
            'memory_gb': 0,
            'supports_bf16': False,
            'supports_fp16': False,
        }
        
        # æ£€æµ‹ GPU
        if torch.cuda.is_available():
            hw_config['device'] = 'cuda'
            hw_config['num_gpus'] = torch.cuda.device_count()
            hw_config['device_name'] = torch.cuda.get_device_name(0)
            hw_config['memory_gb'] = torch.cuda.get_device_properties(0).total_memory / 1e9
            
            # æ£€æµ‹ç²¾åº¦æ”¯æŒ
            hw_config['supports_fp16'] = True
            hw_config['supports_bf16'] = torch.cuda.is_bf16_supported()
            
            # è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç²¾åº¦
            if hw_config['supports_bf16']:
                hw_config['precision'] = 'bf16'
            elif hw_config['supports_fp16']:
                hw_config['precision'] = 'fp16'
            
            # é…ç½®æ˜¾å­˜ç®¡ç†
            if self.config['hardware']['gpu_detection'].get('allow_growth', True):
                torch.cuda.empty_cache()
        
        elif torch.backends.mps.is_available():
            hw_config['device'] = 'mps'
            hw_config['device_name'] = 'Apple Silicon'
            hw_config['supports_fp16'] = True
        
        return hw_config
    
    def _configure_paths(self) -> Dict[str, Path]:
        """
        é…ç½®è·¯å¾„ï¼ˆæ¨¡å‹ã€æ•°æ®ã€è¾“å‡ºç­‰ï¼‰
        æ”¯æŒç¯å¢ƒå˜é‡å’Œç›¸å¯¹è·¯å¾„
        
        Returns:
            è·¯å¾„é…ç½®å­—å…¸
        """
        paths = {}
        path_config = self.config.get('paths', {})
        defaults = path_config.get('defaults', {})
        env_vars = path_config.get('env_vars', {})
        
        # ä¸ºæ¯ä¸ªè·¯å¾„ç±»å‹é…ç½®
        for path_type, default_path in defaults.items():
            # 1. å°è¯•ä»ç¯å¢ƒå˜é‡è¯»å–
            env_var = env_vars.get(path_type)
            if env_var and env_var in os.environ:
                paths[path_type] = Path(os.environ[env_var])
            
            # 2. HPC ç¯å¢ƒç‰¹æ®Šå¤„ç†
            elif self.env_info['type'] == 'hpc':
                username = os.environ.get('USER', 'user')
                hpc_patterns = path_config.get('hpc_patterns', [])
                
                # å°è¯•æ¯ä¸ª HPC è·¯å¾„æ¨¡å¼
                for pattern in hpc_patterns:
                    hpc_path = Path(pattern.format(username=username)) / path_type
                    if hpc_path.exists() or pattern.startswith('/scratch'):
                        paths[path_type] = hpc_path
                        break
                else:
                    # å›é€€åˆ°é»˜è®¤è·¯å¾„
                    paths[path_type] = self.project_root / default_path
            
            # 3. ä½¿ç”¨é»˜è®¤ç›¸å¯¹è·¯å¾„
            else:
                paths[path_type] = self.project_root / default_path
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            paths[path_type].mkdir(parents=True, exist_ok=True)
        
        return paths
    
    def _detect_dependencies(self) -> Dict[str, Any]:
        """
        æ£€æµ‹å¯ç”¨çš„ä¾èµ–åº“
        
        Returns:
            ä¾èµ–ä¿¡æ¯å­—å…¸
        """
        deps = {
            'torch': {
                'available': True,
                'version': torch.__version__,
                'cuda_available': torch.cuda.is_available(),
            },
            'transformers': {
                'available': True,
                'version': transformers.__version__,
            }
        }
        
        # æ£€æµ‹å¯é€‰ä¾èµ–
        optional_deps = {
            'accelerate': 'accelerate',
            'bitsandbytes': 'bitsandbytes',
            'flash_attn': 'flash_attn',
            'deepspeed': 'deepspeed',
            'wandb': 'wandb',
        }
        
        for dep_name, import_name in optional_deps.items():
            try:
                module = __import__(import_name)
                deps[dep_name] = {
                    'available': True,
                    'version': getattr(module, '__version__', 'unknown')
                }
            except ImportError:
                deps[dep_name] = {'available': False}
        
        return deps
    
    def get_device(self) -> torch.device:
        """
        è·å–æ¨èçš„è®¾å¤‡
        
        Returns:
            torch.device å¯¹è±¡
        """
        return torch.device(self.hardware_config['device'])
    
    def get_dtype(self) -> torch.dtype:
        """
        è·å–æ¨èçš„æ•°æ®ç±»å‹
        
        Returns:
            torch.dtype å¯¹è±¡
        """
        precision = self.hardware_config['precision']
        dtype_map = {
            'bf16': torch.bfloat16,
            'fp16': torch.float16,
            'fp32': torch.float32,
        }
        return dtype_map.get(precision, torch.float32)
    
    def get_optimal_batch_size(self, base_size: int = 2) -> Tuple[int, int]:
        """
        æ ¹æ®ç¡¬ä»¶è‡ªåŠ¨è®¡ç®—æœ€ä¼˜ batch size å’Œæ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        
        Args:
            base_size: åŸºç¡€ batch size
            
        Returns:
            (batch_size, gradient_accumulation_steps)
        """
        if not self.config['training'].get('auto_tune', True):
            return base_size, 1
        
        memory_gb = self.hardware_config['memory_gb']
        target_batch = self.config['training']['gradient_accumulation'].get('target_batch_size', 32)
        
        # æ ¹æ®æ˜¾å­˜ä¼°ç®—åˆé€‚çš„ batch size
        if memory_gb >= 40:  # A100 40GB+
            batch_size = 8
        elif memory_gb >= 24:  # RTX 4090, A10
            batch_size = 4
        elif memory_gb >= 16:  # RTX 4080
            batch_size = 2
        elif memory_gb >= 8:   # RTX 4070
            batch_size = 2
        else:
            batch_size = 1
        
        # è®¡ç®—æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        grad_accum = max(1, target_batch // batch_size)
        
        return batch_size, grad_accum
    
    def detect_kv_dimensions(self, model, max_length: int = 32) -> int:
        """
        è¿è¡Œæ—¶åŠ¨æ€æ£€æµ‹æ¨¡å‹ KV Cache çš„å®é™…ç»´åº¦
        
        Args:
            model: åŠ è½½çš„æ¨¡å‹
            max_length: æµ‹è¯•åºåˆ—é•¿åº¦
            
        Returns:
            å®é™… KV ç»´åº¦
        """
        device = self.get_device()
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        test_input = torch.randint(0, 1000, (1, max_length)).to(device)
        
        # å‰å‘ä¼ æ’­è·å– KV cache
        with torch.no_grad():
            outputs = model(test_input, use_cache=True)
            past_key_values = outputs.past_key_values
            
            # è®¡ç®—æ€»ç»´åº¦ï¼ˆæ‰€æœ‰å±‚çš„ç»´åº¦ä¹‹å’Œï¼‰
            total_dim = 0
            for layer_kv in past_key_values:
                k, v = layer_kv
                # k shape: [B, num_heads, seq_len, head_dim]
                B, H, T, D_h = k.shape
                layer_dim = H * D_h
                total_dim += layer_dim
        
        return total_dim
    
    def print_environment_info(self):
        """æ‰“å°ç¯å¢ƒä¿¡æ¯"""
        print("\n" + "="*70)
        print("[Environment Detection Report]")
        print("="*70)
        
        # ç¯å¢ƒç±»å‹
        print(f"\n[Environment Type]: {self.env_info['type'].upper()}")
        print(f"   Platform: {self.env_info['platform']}")
        print(f"   Hostname: {self.env_info['hostname']}")
        print(f"   CPU Cores: {self.env_info['cpu_count']}")
        
        # ç¡¬ä»¶é…ç½®
        print(f"\n[Hardware Configuration]:")
        print(f"   Device: {self.hardware_config['device'].upper()}")
        print(f"   Name: {self.hardware_config['device_name']}")
        if self.hardware_config['num_gpus'] > 0:
            print(f"   GPUs: {self.hardware_config['num_gpus']}")
            print(f"   Memory: {self.hardware_config['memory_gb']:.1f} GB")
        print(f"   Precision: {self.hardware_config['precision'].upper()}")
        print(f"   BF16 Support: {'YES' if self.hardware_config['supports_bf16'] else 'NO'}")
        
        # è·¯å¾„é…ç½®
        print(f"\n[Path Configuration]:")
        for path_type, path in self.paths.items():
            print(f"   {path_type}: {path}")
        
        # ä¾èµ–æ£€æµ‹
        print(f"\n[Dependencies]:")
        for dep_name, dep_info in self.dependencies.items():
            if dep_info['available']:
                version = dep_info.get('version', 'unknown')
                print(f"   [OK] {dep_name} ({version})")
            else:
                print(f"   [X] {dep_name} (not installed)")
        
        print("="*70 + "\n")
    
    def get_training_config(self) -> Dict[str, Any]:
        """
        è·å–å®Œæ•´çš„è®­ç»ƒé…ç½®
        
        Returns:
            è®­ç»ƒé…ç½®å­—å…¸
        """
        batch_size, grad_accum = self.get_optimal_batch_size()
        
        return {
            'device': self.get_device(),
            'dtype': self.get_dtype(),
            'batch_size': batch_size,
            'gradient_accumulation_steps': grad_accum,
            'effective_batch_size': batch_size * grad_accum,
            'paths': self.paths,
            'mixed_precision': self.hardware_config['precision'],
        }


# ====================================================================
# ä¾¿æ·å‡½æ•°
# ====================================================================

def create_environment_adapter(config_path: Optional[str] = None) -> EnvironmentAdapter:
    """
    åˆ›å»ºç¯å¢ƒé€‚é…å™¨ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        EnvironmentAdapter å®ä¾‹
    """
    adapter = EnvironmentAdapter(config_path)
    adapter.print_environment_info()
    return adapter


if __name__ == "__main__":
    # æµ‹è¯•ç¯å¢ƒæ£€æµ‹
    adapter = create_environment_adapter()
    
    print("\nğŸ¯ Recommended Training Configuration:")
    config = adapter.get_training_config()
    for key, value in config.items():
        if key != 'paths':
            print(f"   {key}: {value}")
