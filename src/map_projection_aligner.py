"""
MapProjectionAligner: å®Œæ•´çš„åœ°å›¾æŠ•å½±å¯¹é½å™¨

æ•´åˆå±‚å¯¹é½ã€æ—¶é—´å¯¹é½ã€ç»“æ„åŒ–æŠ•å½±ï¼Œç»Ÿä¸€çš„ Teacher â†’ Student å¯¹é½æ¥å£ã€‚

âœ¨ v4.0 æ›´æ–°ï¼š
- æ·»åŠ  mode å‚æ•°ï¼Œæ”¯æŒ "structured"ï¼ˆæ–°æ–¹æ¡ˆï¼‰/ "flat"ï¼ˆæ—§ baselineï¼‰
- å…¼å®¹æ—§çš„ KVDimensionProjector è·¯å¾„ï¼Œæ–¹ä¾¿ A/B å¯¹æ¯”
- æ˜¾å¼å¤„ç† Qï¼Œæ”¯æŒå®Œæ•´çš„ Q-K-V å¯¹é½
"""
import torch
import torch.nn as nn
from typing import Optional, Dict, Tuple
import sys
import os

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
from .headwise_projector import HeadwiseMapProjector, create_kv_projectors
from .time_warping import TimeWarper, create_default_warper


class MapProjectionAligner(nn.Module):
    """
    åœ°å›¾æŠ•å½±å¯¹é½å™¨ï¼šç»Ÿä¸€çš„ Teacher â†’ Student å¯¹é½æ¥å£
    
    å®Œæˆä¸‰æ­¥å¯¹é½ï¼š
    1. å±‚å¯¹é½ï¼šTeacher layers â†’ Student layers (ratio-based mapping)
    2. æ—¶é—´å¯¹é½ï¼šT_t â†’ T_s (segment-aware warping)
    3. ç»“æ„åŒ–æŠ•å½±ï¼š(H_t, D_t) â†’ (H_s, D_s) (HeadwiseMapProjector)
    
    âœ¨ v4.0 æ–°åŠŸèƒ½ï¼š
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    - mode="structured"ï¼šæ–°çš„åœ°å›¾æŠ•å½±æ–¹æ¡ˆï¼ˆAnti-Flattenï¼‰
    - mode="flat"ï¼šæ—§çš„ flatten + KVDimensionProjector æ–¹æ¡ˆï¼ˆbaselineï¼‰
    
    è¿™æ ·å¯ä»¥ç›´æ¥åœ¨ config é‡Œåˆ‡æ¢åš A/B å¯¹æ¯”ï¼
    
    Args:
        teacher_config: Teacher æ¨¡å‹é…ç½®
        student_config: Student æ¨¡å‹é…ç½®
        mode: "structured" æˆ– "flat"
        layer_mapping_strategy: å±‚æ˜ å°„ç­–ç•¥ ("ratio", "uniform", "skip")
        time_warper: å¯é€‰çš„è‡ªå®šä¹‰ TimeWarper
        share_dim_proj: æ˜¯å¦å…±äº«ç»´åº¦æŠ•å½±ï¼ˆä»… structured æ¨¡å¼ï¼‰
        init_uniform: æ˜¯å¦å‡åŒ€åˆå§‹åŒ–ï¼ˆä»… structured æ¨¡å¼ï¼‰
    
    Example:
        >>> # æ–°æ–¹æ¡ˆï¼ˆåœ°å›¾æŠ•å½±ï¼‰
        >>> aligner = MapProjectionAligner(
        ...     teacher_cfg, student_cfg, mode="structured"
        ... )
        >>> k_s, v_s, q_s = aligner(k_t, v_t, q_t, segment_ids)
        >>> 
        >>> # æ—§æ–¹æ¡ˆï¼ˆflatten baselineï¼‰
        >>> aligner_baseline = MapProjectionAligner(
        ...     teacher_cfg, student_cfg, mode="flat"
        ... )
        >>> k_s, v_s, q_s = aligner_baseline(k_t, v_t, q_t, segment_ids)
    """
    
    def __init__(
        self,
        teacher_config,
        student_config,
        mode: str = "structured",
        layer_mapping_strategy: str = "ratio",
        time_warper: Optional[TimeWarper] = None,
        share_dim_proj: bool = True,
        init_uniform: bool = True
    ):
        super().__init__()
        self.t_cfg = teacher_config
        self.s_cfg = student_config
        self.mode = mode  # "structured" or "flat"
        self.layer_mapping_strategy = layer_mapping_strategy
        
        assert mode in ["structured", "flat"], \
            f"mode å¿…é¡»æ˜¯ 'structured' æˆ– 'flat'ï¼Œå½“å‰: {mode}"
        
        # ===== å…±äº«éƒ¨åˆ†ï¼šå±‚æ˜ å°„ =====
        self.layer_mapping = self.build_layer_mapping()
        
        # ===== å…±äº«éƒ¨åˆ†ï¼šæ—¶é—´å¯¹é½ =====
        self.time_warper = time_warper or create_default_warper()
        
        # ===== æ¨¡å¼åˆ†æ”¯ï¼šåˆå§‹åŒ–æŠ•å½±å™¨ =====
        if mode == "structured":
            # æ–°æ–¹æ¡ˆï¼šHeadwiseMapProjector
            self.proj_k, self.proj_v, self.proj_q = create_kv_projectors(
                teacher_config, student_config,
                share_dim_proj=share_dim_proj,
                init_uniform=init_uniform
            )
        elif mode == "flat":
            # æ—§æ–¹æ¡ˆï¼šKVDimensionProjectorï¼ˆflatten è·¯å¾„ï¼‰
            try:
                from experiments.kv_dimension_projector import KVDimensionProjector
                
                # è®¡ç®— flatten åçš„ç»´åº¦
                H_t = teacher_config.num_attention_heads
                H_s = student_config.num_attention_heads
                D_t = teacher_config.hidden_size // H_t
                D_s = student_config.hidden_size // H_s
                L_t = teacher_config.num_hidden_layers
                L_s = student_config.num_hidden_layers
                
                flat_dim_t = L_t * H_t * D_t
                flat_dim_s = L_s * H_s * D_s
                
                self.kv_flat_projector = KVDimensionProjector(
                    teacher_dim=flat_dim_t,
                    student_dim=flat_dim_s
                )
                
                print(f"âœ… [Flat Mode] ä½¿ç”¨ KVDimensionProjector: {flat_dim_t} â†’ {flat_dim_s}")
            except ImportError:
                raise ImportError(
                    "mode='flat' éœ€è¦ experiments.kv_dimension_projector.KVDimensionProjector\n"
                    "è¯·ç¡®ä¿è¯¥æ¨¡å—å­˜åœ¨ï¼Œæˆ–ä½¿ç”¨ mode='structured'"
                )
    
    def build_layer_mapping(self) -> Dict[int, list]:
        """
        æ„å»ºå±‚æ˜ å°„ï¼šTeacher layer â†’ Student layer(s)
        
        ç­–ç•¥ï¼š
        - "ratio"ï¼šæ¯”ä¾‹æ˜ å°„ l_s = round(l_t * L_s / L_t)
        - "uniform"ï¼šå‡åŒ€æ˜ å°„
        - "skip"ï¼šè·³è¿‡æŸäº›å±‚
        
        Returns:
            mapping: {student_layer_idx: [teacher_layer_indices]}
        """
        L_t = self.t_cfg.num_hidden_layers
        L_s = self.s_cfg.num_hidden_layers
        
        mapping = {}
        
        if self.layer_mapping_strategy == "ratio":
            # æ¯”ä¾‹æ˜ å°„
            for l_t in range(L_t):
                l_s = round(l_t * L_s / L_t)
                l_s = min(l_s, L_s - 1)  # é˜²æ­¢è¶Šç•Œ
                if l_s not in mapping:
                    mapping[l_s] = []
                mapping[l_s].append(l_t)
        
        elif self.layer_mapping_strategy == "uniform":
            # å‡åŒ€æ˜ å°„ï¼šæ¯ä¸ª student å±‚å¯¹åº” ceil(L_t/L_s) ä¸ª teacher å±‚
            step = L_t / L_s
            for l_s in range(L_s):
                start = int(l_s * step)
                end = int((l_s + 1) * step)
                mapping[l_s] = list(range(start, end))
        
        elif self.layer_mapping_strategy == "skip":
            # è·³è¿‡æ˜ å°„ï¼šåªå–ç‰¹å®šå±‚ï¼ˆå¯ä»¥è‡ªå®šä¹‰ï¼‰
            # è¿™é‡Œç®€å•å®ç°ï¼šå–å‡åŒ€åˆ†å¸ƒçš„ L_s å±‚
            indices = torch.linspace(0, L_t - 1, L_s).long().tolist()
            for l_s, l_t in enumerate(indices):
                mapping[l_s] = [l_t]
        
        else:
            raise ValueError(f"æœªçŸ¥çš„å±‚æ˜ å°„ç­–ç•¥: {self.layer_mapping_strategy}")
        
        return mapping
    
    def _apply_layer_map(
        self,
        x_t: torch.Tensor,
        layer_dim: int = 1
    ) -> torch.Tensor:
        """
        åº”ç”¨å±‚æ˜ å°„ï¼šèšåˆ teacher å±‚åˆ° student å±‚
        
        Args:
            x_t: Teacher KVï¼Œå½¢çŠ¶ [B, L_t, H, T, D]
            layer_dim: å±‚ç»´åº¦çš„ç´¢å¼•ï¼ˆé»˜è®¤ 1ï¼‰
        
        Returns:
            x_s: Student å¯¹é½åçš„ KVï¼Œå½¢çŠ¶ [B, L_s, H, T, D]
        """
        B, L_t, H, T, D = x_t.shape
        L_s = self.s_cfg.num_hidden_layers
        
        # åˆ›å»ºè¾“å‡ºå¼ é‡
        x_s = torch.zeros(B, L_s, H, T, D, device=x_t.device, dtype=x_t.dtype)
        
        # æŒ‰æ˜ å°„èšåˆ
        for l_s, teacher_layers in self.layer_mapping.items():
            if len(teacher_layers) == 1:
                # 1å¯¹1æ˜ å°„ï¼šç›´æ¥å¤åˆ¶
                x_s[:, l_s] = x_t[:, teacher_layers[0]]
            else:
                # 1å¯¹å¤šæ˜ å°„ï¼šå¹³å‡
                x_s[:, l_s] = torch.stack([
                    x_t[:, l_t] for l_t in teacher_layers
                ], dim=0).mean(dim=0)
        
        return x_s
    
    def forward(
        self,
        k_t: torch.Tensor,
        v_t: torch.Tensor,
        q_t: torch.Tensor,
        segment_ids: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å®Œæ•´çš„å¯¹é½æµç¨‹ï¼šLayer â†’ Time â†’ Projection
        
        Args:
            k_t: Teacher Keyï¼Œå½¢çŠ¶ [B, L_t, H_t, T_t, D_t]
            v_t: Teacher Valueï¼Œå½¢çŠ¶ [B, L_t, H_t, T_t, D_t]
            q_t: Teacher Queryï¼Œå½¢çŠ¶ [B, L_t, H_t, T_t, D_t]
            segment_ids: æ®µæ ‡ç­¾ï¼Œå½¢çŠ¶ [B, T_t]
        
        Returns:
            k_s, v_s, q_s: Student å¯¹é½åçš„ KVQï¼Œå½¢çŠ¶ [B, L_s, H_s, T_s, D_s]
        """
        B, L_t, H_t, T_t, D_t = k_t.shape
        T_s = segment_ids.shape[1]  # å‡è®¾ segment_ids å·²ç»æ˜¯ç›®æ ‡é•¿åº¦ï¼ˆæˆ–è‡ªåŠ¨æ¨æ–­ï¼‰
        
        # Step 1: å±‚å¯¹é½ [B, L_t, H_t, T_t, D_t] â†’ [B, L_s, H_t, T_t, D_t]
        k_t = self._apply_layer_map(k_t)
        v_t = self._apply_layer_map(v_t)
        q_t = self._apply_layer_map(q_t)
        
        # Step 2: æ—¶é—´å¯¹é½ [B, L_s, H_t, T_t, D_t] â†’ [B, L_s, H_t, T_s, D_t]
        L_s = self.s_cfg.num_hidden_layers
        k_t_warped = []
        v_t_warped = []
        q_t_warped = []
        
        for l in range(L_s):
            # æ³¨æ„ï¼štime_warper æœŸæœ› [B, L, H, T, D]ï¼Œè¿™é‡Œä¼ å…¥ [B, 1, H, T, D]
            k_t_warped.append(self.time_warper(k_t[:, l:l+1], segment_ids, T_s))
            v_t_warped.append(self.time_warper(v_t[:, l:l+1], segment_ids, T_s))
            q_t_warped.append(self.time_warper(q_t[:, l:l+1], segment_ids, T_s))
        
        k_t = torch.cat(k_t_warped, dim=1)  # [B, L_s, H_t, T_s, D_t]
        v_t = torch.cat(v_t_warped, dim=1)
        q_t = torch.cat(q_t_warped, dim=1)
        
        # Step 3: ç»“æ„åŒ–æŠ•å½±ï¼ˆæ¨¡å¼åˆ†æ”¯ï¼‰
        if self.mode == "structured":
            # æ–°æ–¹æ¡ˆï¼šHeadwiseMapProjectorï¼ˆAnti-Flattenï¼‰
            k_s = self.proj_k(k_t)  # [B, L_s, H_s, T_s, D_s]
            v_s = self.proj_v(v_t)
            q_s = self.proj_q(q_t)
        
        elif self.mode == "flat":
            # æ—§æ–¹æ¡ˆï¼šflatten + KVDimensionProjector
            k_s = self._flatten_and_project(k_t)
            v_s = self._flatten_and_project(v_t)
            q_s = self._flatten_and_project(q_t)
        
        return k_s, v_s, q_s
    
    def _flatten_and_project(self, x: torch.Tensor) -> torch.Tensor:
        """
        Flatten + Projectï¼ˆæ—§æ–¹æ¡ˆè·¯å¾„ï¼‰
        
        Args:
            x: [B, L, H_t, T, D_t]
        
        Returns:
            x_proj: [B, L_s, H_s, T, D_s]ï¼ˆunflatten åï¼‰
        """
        B, L, H_t, T, D_t = x.shape
        H_s = self.s_cfg.num_attention_heads
        D_s = self.s_cfg.hidden_size // H_s
        L_s = L  # å±‚æ•°å·²ç»å¯¹é½äº†
        
        # Flatten: [B, L, H_t, T, D_t] â†’ [B, T, L*H_t*D_t]
        x_flat = x.permute(0, 3, 1, 2, 4).reshape(B, T, -1)
        
        # Project: [B, T, L*H_t*D_t] â†’ [B, T, L_s*H_s*D_s]
        x_proj_flat = self.kv_flat_projector(x_flat)
        
        # Unflatten: [B, T, L_s*H_s*D_s] â†’ [B, L_s, H_s, T, D_s]
        x_proj = x_proj_flat.reshape(B, T, L_s, H_s, D_s).permute(0, 2, 3, 1, 4)
        
        return x_proj


# ===== ä¾¿æ·åˆ›å»ºå‡½æ•° =====

def create_structured_aligner(
    teacher_config,
    student_config,
    **kwargs
):
    """
    åˆ›å»ºç»“æ„åŒ–å¯¹é½å™¨ï¼ˆæ–°æ–¹æ¡ˆï¼‰
    """
    return MapProjectionAligner(
        teacher_config, student_config,
        mode="structured",
        **kwargs
    )


def create_flat_aligner(
    teacher_config,
    student_config,
    **kwargs
):
    """
    åˆ›å»º flatten å¯¹é½å™¨ï¼ˆæ—§ baselineï¼‰
    """
    return MapProjectionAligner(
        teacher_config, student_config,
        mode="flat",
        **kwargs
    )


if __name__ == "__main__":
    # ç®€å•æµ‹è¯•
    print("ğŸ§ª æµ‹è¯• MapProjectionAligner")
    
    # æ¨¡æ‹Ÿé…ç½®
    class FakeConfig:
        def __init__(self, num_layers, num_heads, hidden_size):
            self.num_hidden_layers = num_layers
            self.num_attention_heads = num_heads
            self.hidden_size = hidden_size
    
    teacher_cfg = FakeConfig(num_layers=24, num_heads=32, hidden_size=2048)
    student_cfg = FakeConfig(num_layers=12, num_heads=16, hidden_size=1024)
    
    # åˆ›å»ºå¯¹é½å™¨
    aligner_structured = create_structured_aligner(teacher_cfg, student_cfg)
    print(f"âœ… åˆ›å»º structured aligner: mode={aligner_structured.mode}")
    
    # æµ‹è¯•è¾“å…¥
    B, L_t, H_t, T_t, D_t = 2, 24, 32, 100, 64
    k_t = torch.randn(B, L_t, H_t, T_t, D_t)
    v_t = torch.randn(B, L_t, H_t, T_t, D_t)
    q_t = torch.randn(B, L_t, H_t, T_t, D_t)
    
    # Segment IDs
    segment_ids = torch.cat([
        torch.zeros(10, dtype=torch.long),
        torch.ones(80, dtype=torch.long),
        torch.full((10,), 2, dtype=torch.long)
    ]).unsqueeze(0).expand(B, T_t)
    
    print(f"\nè¾“å…¥å½¢çŠ¶:")
    print(f"  k_t: {k_t.shape}")
    print(f"  v_t: {v_t.shape}")
    print(f"  q_t: {q_t.shape}")
    print(f"  segment_ids: {segment_ids.shape}")
    
    # å‰å‘ä¼ æ’­ï¼ˆstructured æ¨¡å¼ï¼‰
    try:
        k_s, v_s, q_s = aligner_structured(k_t, v_t, q_t, segment_ids)
        print(f"\nâœ… Structured æ¨¡å¼è¾“å‡º:")
        print(f"  k_s: {k_s.shape}")
        print(f"  v_s: {v_s.shape}")
        print(f"  q_s: {q_s.shape}")
    except Exception as e:
        print(f"\nâŒ Structured æ¨¡å¼å¤±è´¥: {e}")
    
    # æµ‹è¯• flat æ¨¡å¼ï¼ˆå¦‚æœ KVDimensionProjector å­˜åœ¨ï¼‰
    try:
        aligner_flat = create_flat_aligner(teacher_cfg, student_cfg)
        print(f"\nâœ… åˆ›å»º flat aligner: mode={aligner_flat.mode}")
        
        k_s, v_s, q_s = aligner_flat(k_t, v_t, q_t, segment_ids)
        print(f"\nâœ… Flat æ¨¡å¼è¾“å‡º:")
        print(f"  k_s: {k_s.shape}")
        print(f"  v_s: {v_s.shape}")
        print(f"  q_s: {q_s.shape}")
    except ImportError:
        print("\nâš ï¸ Flat æ¨¡å¼éœ€è¦ KVDimensionProjectorï¼Œè·³è¿‡æµ‹è¯•")
    except Exception as e:
        print(f"\nâŒ Flat æ¨¡å¼å¤±è´¥: {e}")
