"""
ğŸš€ KAVA å®Œå…¨æœ¬åœ°åŒ–è®­ç»ƒè„šæœ¬ - ç®€åŒ–ç¨³å®šç‰ˆ
åŸºäºæ‚¨æœ¬åœ°å·²æœ‰çš„èµ„æºï¼Œç¡®ä¿ 100% èƒ½è¿è¡Œ
é€‚é… RTX 4070 8GB VRAM
"""

import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from datasets import load_dataset
from tqdm import tqdm
import os
import sys

# å¼•å…¥æ ¸å¿ƒç»„ä»¶
from experiments.kv_dimension_projector import KVDimensionProjector
from src.losses import MercatorKVLoss

# --- ğŸ”¥ æœ¬åœ°åŒ–é…ç½® (åŸºäºæ‚¨çš„å®é™…è·¯å¾„) ---
CONFIG = {
    # æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆç›¸å¯¹è·¯å¾„ï¼Œå·²éªŒè¯å­˜åœ¨ï¼‰
    "teacher_path": "local_models/qwen-1.5b-teacher",          
    "student_path": "local_models/qwen-0.5b-student",
    
    # æœ¬åœ°æ•°æ®é›†è·¯å¾„ï¼ˆç›´æ¥æŒ‡å‘ Arrow æ–‡ä»¶ï¼‰
    "dataset_path": "local_data/gsm8k",
    
    # æ˜¾å­˜ä¼˜åŒ–å‚æ•° (8GB VRAM)
    "batch_size": 2,          
    "gradient_accumulation_steps": 16,
    "max_length": 512,
    "lr_projector": 1e-3,
    "lr_student": 5e-5,
    "epochs": 1,
    "save_steps": 200,
    "device": "cuda" if torch.cuda.is_available() else "cpu"
}

def extract_flat_kv(past_key_values, debug=False, use_all_layers=False):
    """
    æå–å¹¶å±•å¹³ KV cache
    
    Args:
        past_key_values: HF æ¨¡å‹è¾“å‡ºçš„ KV cache (tuple of layers)
        debug: æ˜¯å¦æ‰“å°è°ƒè¯•ä¿¡æ¯
        use_all_layers: æ˜¯å¦ä½¿ç”¨æ‰€æœ‰å±‚ï¼ˆç”¨äºé‡åŒ–æ¨¡å‹ï¼‰
    
    Returns:
        k_flat: å±•å¹³åçš„ Key
    """
    if use_all_layers:
        # é‡åŒ–æ¨¡å‹ï¼šèšåˆæ‰€æœ‰å±‚çš„ KV cache
        all_keys = []
        for layer_kv in past_key_values:
            k, v = layer_kv
            if len(k.shape) == 4:
                B, H, T, D_h = k.shape
                k_flat = k.permute(0, 2, 1, 3).contiguous().view(B, T, H * D_h)
            else:
                k_flat = k
            all_keys.append(k_flat)
        
        # æ‹¼æ¥æ‰€æœ‰å±‚: [B, T, num_layers * H * D_h]
        k_combined = torch.cat(all_keys, dim=-1)
        
        if debug:
            print(f"\n[DEBUG extract_flat_kv - All Layers]")
            print(f"   Num layers: {len(past_key_values)}")
            print(f"   Per-layer K shape: {past_key_values[0][0].shape}")
            print(f"   Combined K shape: {k_combined.shape}")
        
        return k_combined
    
    else:
        # æ ‡å‡†æ¨¡å¼ï¼šåªç”¨æœ€åä¸€å±‚
        k, v = past_key_values[-1]
        
        if debug:
            print(f"\n[DEBUG extract_flat_kv - Last Layer]")
            print(f"   Original K shape: {k.shape}")
        
        if len(k.shape) == 4:
            B, H, T, D_h = k.shape
            k_flat = k.permute(0, 2, 1, 3).contiguous().view(B, T, H * D_h)
            
            if debug:
                print(f"   Flattened K shape: {k_flat.shape}")
                print(f"   D_model = H({H}) * D_h({D_h}) = {H * D_h}")
        elif len(k.shape) == 3:
            k_flat = k
            if debug:
                print(f"   K already flattened: {k.shape}")
        else:
            raise ValueError(f"Unexpected K shape: {k.shape}")
        
        return k_flat

def main():
    print("\n" + "ğŸ¯" * 35)
    print("  KAVA Local Training - Simplified & Stable")
    print("  å®Œå…¨æœ¬åœ°åŒ–è®­ç»ƒï¼ˆç®€åŒ–ç¨³å®šç‰ˆï¼‰")
    print("ğŸ¯" * 35 + "\n")
    
    print(f"âš™ï¸ Configuration:")
    print(f"   Teacher: {CONFIG['teacher_path']}")
    print(f"   Student: {CONFIG['student_path']}")
    print(f"   Dataset: {CONFIG['dataset_path']}")
    print(f"   Device: {CONFIG['device']}")
    print(f"   Effective Batch Size: {CONFIG['batch_size']} x {CONFIG['gradient_accumulation_steps']} = {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}")
    
    # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
    os.makedirs("checkpoints", exist_ok=True)
    
    # --- 1. æ•°æ®åŠ è½½ï¼ˆä½¿ç”¨æœ€ç®€å•å¯é çš„æ–¹æ³•ï¼‰---
    print("\nğŸ“š Step 1: Loading Dataset")
    print(f"   Path: {CONFIG['dataset_path']}")
    
    try:
        # ç›´æ¥ä½¿ç”¨ Arrow æ ¼å¼åŠ è½½
        train_arrow = os.path.join(CONFIG['dataset_path'], "train", "*.arrow")
        print(f"   Loading from: {train_arrow}")
        
        dataset = load_dataset(
            "arrow",
            data_files=train_arrow,
            split="train"
        )
        
        print(f"   âœ… Dataset loaded: {len(dataset)} samples")
        
    except Exception as e:
        print(f"   âŒ Dataset loading failed: {e}")
        print("\n   ğŸ’¡ Debug info:")
        print(f"      Checking: {CONFIG['dataset_path']}/train/")
        train_dir = os.path.join(CONFIG['dataset_path'], "train")
        if os.path.exists(train_dir):
            files = os.listdir(train_dir)
            print(f"      Files found: {files}")
        sys.exit(1)
    
    # --- 2. åˆ†è¯å™¨åŠ è½½ ---
    print("\nğŸ”¤ Step 2: Loading Tokenizer")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            CONFIG['student_path'],
            local_files_only=True
        )
        if tokenizer.pad_token is None: 
            tokenizer.pad_token = tokenizer.eos_token
        print(f"   âœ… Tokenizer loaded")
    except Exception as e:
        print(f"   âŒ Tokenizer loading failed: {e}")
        sys.exit(1)
    
    # --- 3. æ•°æ®é¢„å¤„ç† ---
    print("\nğŸ”§ Step 3: Processing Dataset")
    def process(examples):
        texts = [q + "\n" + a for q, a in zip(examples['question'], examples['answer'])]
        return tokenizer(
            texts, 
            truncation=True, 
            padding="max_length", 
            max_length=CONFIG['max_length'],
            return_tensors=None
        )
    
    tokenized_data = dataset.map(
        process, 
        batched=True, 
        remove_columns=dataset.column_names
    )
    tokenized_data.set_format("torch", columns=["input_ids", "attention_mask"])
    
    dataloader = DataLoader(
        tokenized_data, 
        batch_size=CONFIG['batch_size'], 
        shuffle=True
    )
    print(f"   âœ… {len(dataloader)} batches prepared")

    # --- 4. æ¨¡å‹åŠ è½½ ---
    print("\nğŸ¤– Step 4: Loading Models")
    
    try:
        # Teacher (4-bité‡åŒ–)
        print("   Loading Teacher (4-bit quantized)...")
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_quant_type="nf4"
        )
        teacher = AutoModelForCausalLM.from_pretrained(
            CONFIG['teacher_path'], 
            quantization_config=bnb_config, 
            device_map="auto",
            local_files_only=True
        )
        teacher.eval()
        t_dim = teacher.config.hidden_size
        print(f"      âœ… Teacher: d_model={t_dim}")
        
        # Student
        print("   Loading Student (bfloat16)...")
        student = AutoModelForCausalLM.from_pretrained(
            CONFIG['student_path'], 
            torch_dtype=torch.bfloat16, 
            device_map="auto",
            local_files_only=True
        )
        student.train()
        s_dim = student.config.hidden_size
        print(f"      âœ… Student: d_model={s_dim}")
        
    except Exception as e:
        print(f"   âŒ Model loading failed: {e}")
        print("\n   ğŸ’¡ Troubleshooting:")
        print("      1. Check if model folders exist")
        print("      2. Verify config.json in model folders")
        sys.exit(1)

    # --- 5. åŠ¨æ€æ£€æµ‹ Teacher KV ç»´åº¦ ---
    print("\nğŸ” Step 5: Detecting Actual KV Dimensions")
    
    # ç”¨ä¸€ä¸ªå° batch æµ‹è¯•å®é™…çš„ KV ç»´åº¦
    test_input = torch.randint(0, 1000, (1, 32)).to(CONFIG['device'])
    
    with torch.no_grad():
        # Teacher
        t_test_out = teacher(test_input, use_cache=True)
        t_test_kv = extract_flat_kv(t_test_out.past_key_values, use_all_layers=True)
        actual_t_dim = t_test_kv.shape[-1]
        
        # Student
        s_test_out = student(test_input, use_cache=True)
        s_test_kv = extract_flat_kv(s_test_out.past_key_values, use_all_layers=True)
        actual_s_dim = s_test_kv.shape[-1]
    
    print(f"   Detected Teacher KV dim: {actual_t_dim} (config says: {t_dim})")
    print(f"   Detected Student KV dim: {actual_s_dim} (config says: {s_dim})")
    
    # ä½¿ç”¨å®é™…æ£€æµ‹åˆ°çš„ç»´åº¦
    if actual_t_dim != t_dim:
        print(f"   âš ï¸  Using detected dim {actual_t_dim} instead of config {t_dim}")
        t_dim = actual_t_dim
    
    if actual_s_dim != s_dim:
        print(f"   âš ï¸  Using detected dim {actual_s_dim} instead of config {s_dim}")
        s_dim = actual_s_dim
    
    # --- 6. åˆå§‹åŒ– KAVA ç»„ä»¶ï¼ˆä½¿ç”¨å®é™…ç»´åº¦ï¼‰---
    print("\nğŸ—ºï¸ Step 6: Initializing KAVA Components")
    
    projector = KVDimensionProjector(
        teacher_configs={"local_teacher": {"d_model": t_dim}},
        student_d_model=s_dim,
        mlp_ratio=1.0,
        dropout=0.1
    ).to(CONFIG['device']).to(torch.bfloat16)
    
    loss_fn = MercatorKVLoss(alpha=1.0, beta=0.01).to(CONFIG['device'])
    
    print(f"   Projector: {t_dim} -> {s_dim}")
    print(f"   Loss: Mercator (alpha=1.0, beta=0.01)")
    
    optimizer = optim.AdamW([
        {'params': student.parameters(), 'lr': CONFIG['lr_student']},
        {'params': projector.parameters(), 'lr': CONFIG['lr_projector']}
    ])
    
    print(f"   Optimizer: AdamW")
    print(f"      Student LR: {CONFIG['lr_student']}")
    print(f"      Projector LR: {CONFIG['lr_projector']}")

    # --- 6. è®­ç»ƒå¾ªç¯ ---
    print("\n" + "=" * 70)
    print("ğŸ¯ Training Start - Monitor 'CosSim' (Target: >0.90)")
    print("=" * 70 + "\n")
    
    global_step = 0
    progress = tqdm(dataloader, desc="Training")
    first_batch = True  # ç”¨äºè°ƒè¯•ç¬¬ä¸€ä¸ª batch
    
    try:
        for i, batch in enumerate(progress):
            input_ids = batch['input_ids'].to(CONFIG['device'])
            mask = batch['attention_mask'].to(CONFIG['device'])
            
            # Teacher Forward (No Grad) - ä½¿ç”¨æ‰€æœ‰å±‚
            with torch.no_grad():
                t_out = teacher(input_ids, attention_mask=mask, use_cache=True)
                t_kv = extract_flat_kv(t_out.past_key_values, debug=first_batch, use_all_layers=True)
                # è½¬æ¢ä¸º bfloat16 ä»¥åŒ¹é… Projector
                t_kv = t_kv.to(torch.bfloat16)
                
            # Student Forward - ä¹Ÿä½¿ç”¨æ‰€æœ‰å±‚ä¿æŒä¸€è‡´æ€§
            s_out = student(input_ids, attention_mask=mask, use_cache=True)
            s_kv = extract_flat_kv(s_out.past_key_values, debug=first_batch, use_all_layers=True)
            # ç¡®ä¿ç±»å‹ä¸€è‡´
            s_kv = s_kv.to(torch.bfloat16)
            
            if first_batch:
                print(f"\n[First Batch Debug Info]")
                print(f"   Input shape: {input_ids.shape}")
                print(f"   Teacher KV shape: {t_kv.shape} (expected: [B, T, {t_dim}])")
                print(f"   Student KV shape: {s_kv.shape} (expected: [B, T, {s_dim}])")
                print(f"   Projector config: {t_dim} -> {s_dim}")
                first_batch = False
            
            # Projection & Loss
            t_proj, _ = projector.project_teacher_kv("local_teacher", t_kv, t_kv)
            loss, metrics = loss_fn(s_kv, t_proj)
            
            # æ¢¯åº¦ç´¯ç§¯
            loss = loss / CONFIG['gradient_accumulation_steps']
            loss.backward()
            
            if (i + 1) % CONFIG['gradient_accumulation_steps'] == 0:
                torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)
                optimizer.step()
                optimizer.zero_grad()
                global_step += 1
                
                # æ ¸å¿ƒç›‘æ§
                actual_loss = loss.item() * CONFIG['gradient_accumulation_steps']
                cos_sim = metrics['cos_sim']
                
                # çŠ¶æ€åˆ¤æ–­
                if cos_sim >= 0.95:
                    status = "âœ… Excellent"
                elif cos_sim >= 0.90:
                    status = "ğŸ¯ Great"
                elif cos_sim >= 0.70:
                    status = "ğŸ“ˆ Good"
                elif cos_sim >= 0.50:
                    status = "âš ï¸ Learning"
                else:
                    status = "ğŸ”„ Adapting"
                
                progress.set_postfix({
                    "Loss": f"{actual_loss:.4f}", 
                    "CosSim": f"{cos_sim:.4f}",
                    "Status": status
                })
                
                # æ¯ 50 æ­¥è¯¦ç»†æŠ¥å‘Š
                if global_step % 50 == 0:
                    print(f"\n[Step {global_step:04d}] Loss: {actual_loss:.4f} | CosSim: {cos_sim:.4f} {status}")
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if global_step % CONFIG['save_steps'] == 0:
                    checkpoint_path = f"checkpoints/proj_step_{global_step}.pth"
                    torch.save(projector.state_dict(), checkpoint_path)
                    print(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")

        print("\n" + "=" * 70)
        print("âœ… Training Complete!")
        print("=" * 70)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        torch.save(projector.state_dict(), "final_projector.pth")
        student.save_pretrained("final_student")
        
        print("\nğŸ’¾ Final models saved:")
        print("   - final_projector.pth")
        print("   - final_student/")
        print("\nğŸ‰ All Done!")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Training interrupted by user")
        print("ğŸ’¾ Saving emergency checkpoint...")
        torch.save(projector.state_dict(), "checkpoints/emergency_projector.pth")
        print("âœ… Emergency checkpoint saved")
    
    except Exception as e:
        print(f"\n\nâŒ Error occurred: {e}")
        print("\nğŸ’¡ Full error traceback:")
        import traceback
        traceback.print_exc()
        print("\nğŸ’¾ Saving emergency checkpoint...")
        torch.save(projector.state_dict(), "checkpoints/emergency_projector.pth")

if __name__ == "__main__":
    print("\n" + "ğŸš€" * 35)
    print("Starting KAVA Training with Local Resources")
    print("ğŸš€" * 35)
    
    # ç¯å¢ƒæ£€æŸ¥
    print("\nğŸ“‹ Environment Check:")
    print(f"   Python: {sys.version.split()[0]}")
    print(f"   PyTorch: {torch.__version__}")
    print(f"   CUDA Available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   CUDA Device: {torch.cuda.get_device_name(0)}")
        print(f"   CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    main()
