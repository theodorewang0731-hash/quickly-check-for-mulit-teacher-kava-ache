"""
Shape Verification Tests for Multi-Teacher KV Alignment
========================================================

Mock data tests to verify the complete alignment pipeline WITHOUT loading real models.
Tests Head Flatten -> Layer Alignment -> Time Resampling -> Dimension Projection.

Author: Quick Check Team
Date: 2025-01-26
"""

import torch
import numpy as np
import sys
sys.path.append(".")

from experiments.alignment_v2 import (
    CKALayerMapper,
    SequenceSegment,
    resample_kv_with_interpolation,
)
from experiments.kv_dimension_projector import (
    KVDimensionProjector,
    flatten_kv_heads,
    unflatten_kv_heads,
)


def test_head_flatten():
    """Test 1: Head dimension flattening"""
    print("\n" + "="*80)
    print("Test 1: Head Dimension Flattening")
    print("="*80)
    
    # Mock teacher KV with head dimension
    B, L, H, T, d_head = 2, 28, 32, 80, 128
    teacher_kv_with_heads = torch.randn(B, L, H, T, d_head)
    
    print(f"Input shape:  {list(teacher_kv_with_heads.shape)}")
    print(f"  B={B}, L={L}, H={H}, T={T}, d_head={d_head}")
    
    # Flatten: [B, L, H, T, d_head] -> [B, L, T, H*d_head]
    teacher_kv_flat = flatten_kv_heads(teacher_kv_with_heads, H, d_head)
    
    expected_d_model = H * d_head
    print(f"Output shape: {list(teacher_kv_flat.shape)}")
    print(f"  Expected: [B={B}, L={L}, T={T}, d_model={expected_d_model}]")
    
    # Verify shape
    assert teacher_kv_flat.shape == (B, L, T, expected_d_model), \
        f"Expected {(B, L, T, expected_d_model)}, got {teacher_kv_flat.shape}"
    
    # Verify reversibility
    teacher_kv_unflat = unflatten_kv_heads(teacher_kv_flat, H, d_head)
    assert teacher_kv_unflat.shape == teacher_kv_with_heads.shape, \
        "Unflatten should reverse flatten"
    assert torch.allclose(teacher_kv_unflat, teacher_kv_with_heads), \
        "Data should be preserved after flatten/unflatten"
    
    print("âœ“ Head flattening correct")
    print("âœ“ Flatten/unflatten reversible")
    print("âœ“ No data corruption")
    
    return teacher_kv_flat


def test_layer_alignment():
    """Test 2: CKA-based layer alignment"""
    print("\n" + "="*80)
    print("Test 2: Layer Alignment (CKA Top-k Mapping)")
    print("="*80)
    
    # Mock setup
    B = 2
    student_layers = 12
    teacher_layers = 28
    T = 80
    d_model = 4096  # Teacher d_model
    
    print(f"Student: {student_layers} layers")
    print(f"Teacher: {teacher_layers} layers")
    
    # Create layer mapper
    layer_mapper = CKALayerMapper(
        student_num_layers=student_layers,
        teacher_num_layers=teacher_layers,
        top_k=2
    )
    
    # Mock similarity matrix (random but valid)
    layer_mapper.similarity_matrix = np.random.rand(student_layers, teacher_layers)
    layer_mapper.build_layer_mapping()
    
    # Mock teacher KV for all layers
    teacher_kvs = []
    for layer_idx in range(teacher_layers):
        K = torch.randn(B, T, d_model)
        V = torch.randn(B, T, d_model)
        teacher_kvs.append((K, V))
    
    print(f"Teacher KV per layer: K/V shape {list(teacher_kvs[0][0].shape)}")
    
    # Get aligned KV for student layer 5
    student_layer_idx = 5
    K_aligned, V_aligned = layer_mapper.get_aligned_teacher_kv(
        student_layer_idx, teacher_kvs
    )
    
    print(f"\nFor student layer {student_layer_idx}:")
    print(f"  Aligned K shape: {list(K_aligned.shape)}")
    print(f"  Aligned V shape: {list(V_aligned.shape)}")
    print(f"  Mapping: {layer_mapper.layer_mapping[student_layer_idx]}")
    
    # Verify shape (should keep [B, T, d_model])
    assert K_aligned.shape == (B, T, d_model), \
        f"Expected K shape {(B, T, d_model)}, got {K_aligned.shape}"
    assert V_aligned.shape == (B, T, d_model), \
        f"Expected V shape {(B, T, d_model)}, got {V_aligned.shape}"
    
    print("âœ“ Layer alignment preserves shape")
    print("âœ“ Top-k weighted combination works")
    
    return K_aligned, V_aligned


def test_time_resampling():
    """Test 3: Time dimension resampling"""
    print("\n" + "="*80)
    print("Test 3: Time Dimension Resampling")
    print("="*80)
    
    # Mock teacher KV
    B = 2
    teacher_length = 80
    student_length = 50
    d_model = 4096
    
    teacher_K = torch.randn(B, teacher_length, d_model)
    teacher_V = torch.randn(B, teacher_length, d_model)
    
    print(f"Teacher KV: K/V shape {list(teacher_K.shape)}")
    print(f"  Teacher length: {teacher_length}")
    print(f"  Student length: {student_length}")
    
    # Test 3a: Global resampling (no segments)
    print("\n[Test 3a] Global resampling (no segments)")
    K_resampled_global = resample_kv_with_interpolation(
        teacher_K, student_length, None, None
    )
    V_resampled_global = resample_kv_with_interpolation(
        teacher_V, student_length, None, None
    )
    
    print(f"  Output K shape: {list(K_resampled_global.shape)}")
    print(f"  Output V shape: {list(V_resampled_global.shape)}")
    
    assert K_resampled_global.shape == (B, student_length, d_model), \
        f"Expected {(B, student_length, d_model)}, got {K_resampled_global.shape}"
    assert V_resampled_global.shape == (B, student_length, d_model), \
        f"Expected {(B, student_length, d_model)}, got {V_resampled_global.shape}"
    
    print("  âœ“ Global resampling works")
    
    # Test 3b: Segment-aware resampling
    print("\n[Test 3b] Segment-aware resampling")
    
    teacher_segments = [
        SequenceSegment("prompt", 0, 15, 15),
        SequenceSegment("reasoning", 15, 70, 55),
        SequenceSegment("answer", 70, 80, 10)
    ]
    
    student_segments = [
        SequenceSegment("prompt", 0, 10, 10),
        SequenceSegment("reasoning", 10, 40, 30),
        SequenceSegment("answer", 40, 50, 10)
    ]
    
    print(f"  Teacher segments: {[(s.start, s.end) for s in teacher_segments]}")
    print(f"  Student segments: {[(s.start, s.end) for s in student_segments]}")
    
    K_resampled_seg = resample_kv_with_interpolation(
        teacher_K, student_length, teacher_segments, student_segments
    )
    V_resampled_seg = resample_kv_with_interpolation(
        teacher_V, student_length, teacher_segments, student_segments
    )
    
    print(f"  Output K shape: {list(K_resampled_seg.shape)}")
    print(f"  Output V shape: {list(V_resampled_seg.shape)}")
    
    assert K_resampled_seg.shape == (B, student_length, d_model), \
        f"Expected {(B, student_length, d_model)}, got {K_resampled_seg.shape}"
    assert V_resampled_seg.shape == (B, student_length, d_model), \
        f"Expected {(B, student_length, d_model)}, got {V_resampled_seg.shape}"
    
    print("  âœ“ Segment-aware resampling works")
    
    return K_resampled_global, V_resampled_global


def test_dimension_projection():
    """Test 4: Dimension projection"""
    print("\n" + "="*80)
    print("Test 4: Dimension Projection (d_teacher -> d_student)")
    print("="*80)
    
    # Mock setup
    B = 2
    L = 1  # Single layer for simplicity
    T = 50  # After time resampling
    d_teacher = 4096
    d_student = 2048
    
    # Create projector
    teacher_configs = {
        "MockTeacher-70B": {"d_model": d_teacher, "num_layers": 28}
    }
    
    projector = KVDimensionProjector(
        teacher_configs=teacher_configs,
        student_d_model=d_student,
        init_method="xavier",
        trainable=True
    )
    
    print(f"Projector created:")
    print(f"  Teacher d_model: {d_teacher}")
    print(f"  Student d_model: {d_student}")
    print(f"  Parameters: {projector.count_parameters():,}")
    
    # Mock teacher KV (after time resampling)
    teacher_K = torch.randn(B, L, T, d_teacher)
    teacher_V = torch.randn(B, L, T, d_teacher)
    
    print(f"\nInput KV shape: {list(teacher_K.shape)}")
    
    # Project
    K_projected, V_projected = projector.project_teacher_kv(
        "MockTeacher-70B", teacher_K, teacher_V
    )
    
    print(f"Output K shape: {list(K_projected.shape)}")
    print(f"Output V shape: {list(V_projected.shape)}")
    
    # Verify shape
    assert K_projected.shape == (B, L, T, d_student), \
        f"Expected {(B, L, T, d_student)}, got {K_projected.shape}"
    assert V_projected.shape == (B, L, T, d_student), \
        f"Expected {(B, L, T, d_student)}, got {V_projected.shape}"
    
    print("âœ“ Dimension projection correct")
    print("âœ“ Learnable projection works")
    
    return K_projected, V_projected


def test_complete_pipeline():
    """Test 5: Complete end-to-end pipeline"""
    print("\n" + "="*80)
    print("Test 5: COMPLETE PIPELINE - End to End Shape Verification")
    print("="*80)
    
    # Configuration
    B = 2
    
    # Teacher config (mock 70B model)
    teacher_layers = 28
    teacher_heads = 32
    teacher_head_dim = 128
    teacher_length = 80
    teacher_d_model = teacher_heads * teacher_head_dim  # 4096
    
    # Student config (mock 7B model)
    student_layers = 12
    student_length = 50
    student_d_model = 2048
    
    print("Configuration:")
    print(f"  Teacher: {teacher_layers} layers, {teacher_heads} heads, "
          f"{teacher_length} tokens, d_model={teacher_d_model}")
    print(f"  Student: {student_layers} layers, {student_length} tokens, "
          f"d_model={student_d_model}")
    
    # Step 1: Generate mock teacher KV with head dimension
    print("\n[Step 1] Mock Teacher KV Generation")
    teacher_K_with_heads = torch.randn(B, teacher_layers, teacher_heads, 
                                       teacher_length, teacher_head_dim)
    teacher_V_with_heads = torch.randn(B, teacher_layers, teacher_heads, 
                                       teacher_length, teacher_head_dim)
    print(f"  Teacher K (with heads): {list(teacher_K_with_heads.shape)}")
    print(f"  Format: [B, L, H, T, d_head]")
    
    # Step 2: Flatten heads
    print("\n[Step 2] Flatten Head Dimension")
    teacher_K_flat = flatten_kv_heads(teacher_K_with_heads, teacher_heads, teacher_head_dim)
    teacher_V_flat = flatten_kv_heads(teacher_V_with_heads, teacher_heads, teacher_head_dim)
    print(f"  After flatten: {list(teacher_K_flat.shape)}")
    print(f"  Format: [B, L, T, d_model]")
    assert teacher_K_flat.shape == (B, teacher_layers, teacher_length, teacher_d_model)
    
    # Step 3: Layer alignment (CKA top-k)
    print("\n[Step 3] Layer Alignment (CKA Top-k)")
    layer_mapper = CKALayerMapper(student_layers, teacher_layers, top_k=2)
    layer_mapper.similarity_matrix = np.random.rand(student_layers, teacher_layers)
    layer_mapper.build_layer_mapping()
    
    # For student layer 5
    student_layer_idx = 5
    teacher_kv_list = [(teacher_K_flat[:, i], teacher_V_flat[:, i]) 
                       for i in range(teacher_layers)]
    
    K_layer_aligned, V_layer_aligned = layer_mapper.get_aligned_teacher_kv(
        student_layer_idx, teacher_kv_list
    )
    print(f"  After layer alignment: {list(K_layer_aligned.shape)}")
    print(f"  Format: [B, T, d_model]")
    print(f"  Student L{student_layer_idx} maps to teacher layers: "
          f"{layer_mapper.layer_mapping[student_layer_idx]}")
    assert K_layer_aligned.shape == (B, teacher_length, teacher_d_model)
    
    # Step 4: Time resampling
    print("\n[Step 4] Time Dimension Resampling")
    K_time_aligned = resample_kv_with_interpolation(
        K_layer_aligned, student_length, None, None
    )
    V_time_aligned = resample_kv_with_interpolation(
        V_layer_aligned, student_length, None, None
    )
    print(f"  After time resampling: {list(K_time_aligned.shape)}")
    print(f"  Format: [B, T_student, d_model]")
    assert K_time_aligned.shape == (B, student_length, teacher_d_model)
    
    # Step 5: Dimension projection
    print("\n[Step 5] Dimension Projection")
    teacher_configs = {
        "MockTeacher-70B": {"d_model": teacher_d_model, "num_layers": teacher_layers}
    }
    projector = KVDimensionProjector(
        teacher_configs=teacher_configs,
        student_d_model=student_d_model,
        init_method="xavier",
        trainable=True
    )
    
    # Add layer dimension for projector
    K_with_layer = K_time_aligned.unsqueeze(1)  # [B, 1, T, d]
    V_with_layer = V_time_aligned.unsqueeze(1)
    
    K_final, V_final = projector.project_teacher_kv(
        "MockTeacher-70B", K_with_layer, V_with_layer
    )
    
    # Remove layer dimension
    K_final = K_final.squeeze(1)  # [B, T, d_student]
    V_final = V_final.squeeze(1)
    
    print(f"  After projection: {list(K_final.shape)}")
    print(f"  Format: [B, T_student, d_student]")
    
    # Final verification
    print("\n" + "="*80)
    print("FINAL VERIFICATION")
    print("="*80)
    print(f"Input:  Teacher KV [B={B}, L={teacher_layers}, H={teacher_heads}, "
          f"T={teacher_length}, d_head={teacher_head_dim}]")
    print(f"Output: Student KV [B={B}, T={student_length}, d_student={student_d_model}]")
    print(f"Actual output shape: K={list(K_final.shape)}, V={list(V_final.shape)}")
    
    # Critical assertion
    expected_shape = (B, student_length, student_d_model)
    assert K_final.shape == expected_shape, \
        f"K shape mismatch! Expected {expected_shape}, got {K_final.shape}"
    assert V_final.shape == expected_shape, \
        f"V shape mismatch! Expected {expected_shape}, got {V_final.shape}"
    
    print("\nâœ“âœ“âœ“ ALL SHAPE CHECKS PASSED âœ“âœ“âœ“")
    print("âœ“ Head flattening correct")
    print("âœ“ Layer alignment preserves dimensions")
    print("âœ“ Time resampling correct")
    print("âœ“ Dimension projection correct")
    print("âœ“ Final output shape matches student expectation")
    
    return K_final, V_final


def test_common_pitfalls():
    """Test 6: Common pitfalls and edge cases"""
    print("\n" + "="*80)
    print("Test 6: Common Pitfalls & Edge Cases")
    print("="*80)
    
    # Pitfall 1: view() vs reshape() with non-contiguous tensors
    print("\n[Pitfall 1] Non-contiguous tensor handling")
    B, L, H, T, d_head = 2, 12, 16, 50, 64
    kv = torch.randn(B, L, H, T, d_head)
    
    # After transpose, tensor is non-contiguous
    kv_transposed = kv.transpose(2, 3)  # [B, L, T, H, d_head]
    print(f"  After transpose, is_contiguous: {kv_transposed.is_contiguous()}")
    
    # flatten_kv_heads should handle this with .contiguous()
    kv_flat = flatten_kv_heads(kv, H, d_head)
    print(f"  Flatten works even with non-contiguous input: âœ“")
    
    # Pitfall 2: Dimension order after flatten
    print("\n[Pitfall 2] Dimension order verification")
    # Ensure H*d_head is in the LAST dimension, not mixed with T
    assert kv_flat.shape[-1] == H * d_head, "d_model must be last dimension"
    assert kv_flat.shape[-2] == T, "Sequence length must be second-to-last"
    print(f"  Dimension order correct: [..., T={T}, d_model={H*d_head}] âœ“")
    
    # Pitfall 3: Batch dimension preservation
    print("\n[Pitfall 3] Batch dimension preservation")
    assert kv_flat.shape[0] == B, "Batch dimension must be preserved"
    print(f"  Batch dimension preserved: B={B} âœ“")
    
    # Pitfall 4: Empty/single sequence handling
    print("\n[Pitfall 4] Edge case - single token sequence")
    single_token_kv = torch.randn(B, L, H, 1, d_head)  # T=1
    single_flat = flatten_kv_heads(single_token_kv, H, d_head)
    assert single_flat.shape == (B, L, 1, H*d_head)
    print(f"  Single token sequence handled: âœ“")
    
    # Pitfall 5: Very long sequence
    print("\n[Pitfall 5] Edge case - very long sequence")
    long_seq_kv = torch.randn(1, 1, 8, 2048, 128)  # T=2048
    long_flat = flatten_kv_heads(long_seq_kv, 8, 128)
    assert long_flat.shape == (1, 1, 2048, 8*128)
    print(f"  Long sequence (T=2048) handled: âœ“")
    
    print("\nâœ“ All common pitfalls handled correctly")


def run_all_tests():
    """Run all shape verification tests"""
    print("\n" + "â–ˆ"*80)
    print("â–ˆ" + " "*78 + "â–ˆ")
    print("â–ˆ" + "  SHAPE VERIFICATION TEST SUITE - Mock Data (No Real Models)".center(78) + "â–ˆ")
    print("â–ˆ" + " "*78 + "â–ˆ")
    print("â–ˆ"*80)
    
    try:
        # Individual component tests
        test_head_flatten()
        test_layer_alignment()
        test_time_resampling()
        test_dimension_projection()
        
        # Complete pipeline test
        test_complete_pipeline()
        
        # Edge cases
        test_common_pitfalls()
        
        # Final summary
        print("\n" + "â–ˆ"*80)
        print("â–ˆ" + " "*78 + "â–ˆ")
        print("â–ˆ" + "  âœ“âœ“âœ“ ALL TESTS PASSED âœ“âœ“âœ“".center(78) + "â–ˆ")
        print("â–ˆ" + " "*78 + "â–ˆ")
        print("â–ˆ"*80)
        print("\nSummary:")
        print("  âœ“ Head flattening works correctly")
        print("  âœ“ Layer alignment (CKA top-k) preserves shapes")
        print("  âœ“ Time resampling (global & segment-aware) works")
        print("  âœ“ Dimension projection (d_teacher -> d_student) works")
        print("  âœ“ Complete pipeline produces correct output shape")
        print("  âœ“ Common pitfalls handled (non-contiguous, edge cases)")
        print("\nReady for real model integration! ðŸš€")
        
        return True
        
    except AssertionError as e:
        print("\n" + "â–ˆ"*80)
        print("â–ˆ" + " "*78 + "â–ˆ")
        print("â–ˆ" + "  âœ—âœ—âœ— TEST FAILED âœ—âœ—âœ—".center(78) + "â–ˆ")
        print("â–ˆ" + " "*78 + "â–ˆ")
        print("â–ˆ"*80)
        print(f"\nError: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = run_all_tests()
    exit(0 if success else 1)
